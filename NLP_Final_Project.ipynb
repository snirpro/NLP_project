{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8bPm6DxMYH8w",
        "WZYIePpWYaA3",
        "HQx-UkZJZsJi",
        "OkdAsk3vZ3xM",
        "13KoSo3Se_Yo",
        "xitSXj6Vo1T2",
        "mIq0IvWOpUwO",
        "Cb-Nt8ZoqFuy",
        "S_Tvi4kKqSYv",
        "Epawb-rosV-k",
        "5vd79T6OscsV",
        "KrRaEEuEse8y",
        "9p0IMlMJskFz",
        "7bv_KymxzcAm",
        "Ij-_uIhPzfvM",
        "aLZ3xUKjzlC1",
        "TGsZu0Mfzn-8",
        "IWKWR5iuzpSF",
        "fJQ9b9yTztGi",
        "DKb4UHdhzwvZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#××’×™×©×™×\n",
        "###×©× ×™×¨ ×¤×¨×™××•×‘×™×¥ 211518139\n",
        "###×™×•×¡×£ ×¡×¨×•×¨ 209452945\n",
        "###××™×ª×™ ×××•×¡ 313348104\n",
        "###×“×•×ª×Ÿ ×—×–×•×˜ 315779926"
      ],
      "metadata": {
        "id": "a-m9a6NDLn3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#×—×œ×§ ×¨××©×•×Ÿ: ×”×›× ×” ×•×”×›×¨×•×ª ×¢× ×”×“××˜×”"
      ],
      "metadata": {
        "id": "8bPm6DxMYH8w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×”××¨×ª ×”×§×•×‘×¥ ×œ×¤×•×¨××˜ csv"
      ],
      "metadata": {
        "id": "WZYIePpWYaA3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmpZ8VnGVgaj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "aa6940df-86a9-4dbd-b662-a1c0b1627081"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Restaurant_Reviews.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the TSV file\n",
        "tsv_file_path = '/content/drive/MyDrive/Restaurant_Reviews.tsv'\n",
        "df = pd.read_csv(tsv_file_path, sep='\\t')\n",
        "\n",
        "# Save as CSV\n",
        "csv_file_path = '/content/drive/MyDrive/Restaurant_Reviews.csv'\n",
        "df.to_csv(csv_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×§×•×“ ×©××•×¦× ×›××” ×‘×™×§×•×¨×•×ª ×—×™×•×‘×™×•×ª ×™×© ×•×›××” ×©×œ×™×œ×™×•×ª"
      ],
      "metadata": {
        "id": "HQx-UkZJZsJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count positive and negative reviews based on the 'Liked' column\n",
        "positive_count = (df['Liked'] == 1).sum()\n",
        "negative_count = (df['Liked'] == 0).sum()\n",
        "\n",
        "print(\"Number of positive reviews:\", positive_count)\n",
        "print(\"Number of negative reviews:\", negative_count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wZvdgJVZQuC",
        "outputId": "450b0408-6918-4fc6-9287-caca79c76936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of positive reviews: 500\n",
            "Number of negative reviews: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×§×•×“ ××©×¨ ××•×¦× ××ª ×”××•×¨×š ×”×××•×¦×¢ ×œ×‘×™×§×•×¨×ª ×—×™×•×‘×™×ª ×•×œ×‘×™×§×•×¨×ª ×©×œ×™×œ×™×ª"
      ],
      "metadata": {
        "id": "OkdAsk3vZ3xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the length of each review\n",
        "df['Review_Length'] = df['Review'].apply(len)\n",
        "\n",
        "# Compute average length for positive and negative reviews\n",
        "avg_length_positive = df[df['Liked'] == 1]['Review_Length'].mean()\n",
        "avg_length_negative = df[df['Liked'] == 0]['Review_Length'].mean()\n",
        "\n",
        "print(\"Average length of positive reviews(in characters):\", avg_length_positive)\n",
        "print(\"Average length of negative reviews(in characters):\", avg_length_negative)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmLzU2jeZ8zB",
        "outputId": "4e3dd978-0d3b-4cab-9ae9-a4ff70e71240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average length of positive reviews(in characters): 55.898\n",
            "Average length of negative reviews(in characters): 60.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##× ×™×ª×Ÿ ×œ×”×¡×™×§ ××Ÿ ×”×¡×¢×™×¤×™× ×”×§×•×“××™× ×©×§×•×‘×¥ ×”××™×“×¢ ×˜×•×‘ ×××—×¨ ×•×”×•× ××›×™×œ ×›××•×ª ××›×•×‘×“×ª ×©×œ ×‘×™×§×•×¨×•×ª ×‘×¢×œ×™ ×”×¨×‘×” ××œ×œ ××– ×™×© ×”×¨×‘×” ×××” ×œ×œ××•×“ ×•×”×‘×™×§×•×¨×•×ª ××’×•×•× ×•×ª"
      ],
      "metadata": {
        "id": "RV_iSuMLaXuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##××¦×™××ª 5 ×”××™×œ×™× ×”× ×¤×•×¦×•×ª ×‘×™×•×ª×¨ ×‘×‘×‘×™×§×•×¨×•×ª ×”×©×œ×™×œ×™×•×ª"
      ],
      "metadata": {
        "id": "13KoSo3Se_Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the small English model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Filter only negative reviews\n",
        "negative_reviews = df[df['Liked'] == 0]['Review']\n",
        "\n",
        "# Process all negative reviews into one large text\n",
        "all_negative_text = \" \".join(negative_reviews)\n",
        "\n",
        "# Use spaCy to process the text\n",
        "doc = nlp(all_negative_text)\n",
        "\n",
        "# Extract meaningful words (nouns, adjectives, verbs) excluding stop words and punctuation\n",
        "tokens = [token.lemma_.lower() for token in doc\n",
        "          if token.is_alpha and not token.is_stop and token.pos_ in [\"NOUN\", \"ADJ\", \"VERB\"]]\n",
        "\n",
        "# Count the most common words\n",
        "common_words = Counter(tokens).most_common(5)\n",
        "print(\"The most common words in negative reviews are:\")\n",
        "for word, count in common_words:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw14L9bxf9Hn",
        "outputId": "9fb3a84e-dbc8-475e-d42f-dea528421f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most common words in negative reviews are:\n",
            "food: 67\n",
            "place: 52\n",
            "service: 38\n",
            "bad: 32\n",
            "time: 29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###××” × ×™×ª×Ÿ ×œ×”×¡×™×§ ××”××™×œ×™×\n",
        "\n",
        "1.   food - ×‘×™×§×•×¨×•×ª ×¢×œ ××™×›×•×ª ×”××•×›×œ\n",
        "2.   place - ×‘×™×§×•×¨×•×ª ×¢×œ ×”××§×•×\n",
        "3.   service - ×‘×™×§×•×¨×•×ª ×¢×œ ×©×™×¨×•×ª\n",
        "4.   bad - ×—×•×•×ª ×“×¢×ª\n",
        "5.   time - ×‘×™×§×•×¨×•×ª ×›× ×¨××” ×¢×œ ×–××Ÿ ×”×”×’×©×”\n",
        "\n"
      ],
      "metadata": {
        "id": "H-JDxMN6gZRY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##××¦×™××ª ×”×‘×™×§×•×¨×ª ×”××¨×•×›×” ×‘×™×•×ª×¨"
      ],
      "metadata": {
        "id": "xitSXj6Vo1T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the row with the maximum review length\n",
        "longest_review_row = df.loc[df['Review_Length'].idxmax()]\n",
        "\n",
        "longest_review_text = longest_review_row['Review']\n",
        "longest_review_label = 'Positive' if longest_review_row['Liked'] == 1 else 'Negative'\n",
        "\n",
        "print(\"The longest review is:\")\n",
        "print(longest_review_text)\n",
        "print(\"Label:\", longest_review_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvRUliObo4_2",
        "outputId": "c3f6f319-0d5f-47f8-8be8-d85d1644e3bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The longest review is:\n",
            "The problem I have is that they charge $11.99 for a sandwich that is no bigger than a Subway sub (which offers better and more amount of vegetables).\n",
            "Label: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##××¦×™××ª NER ×•×¡×•×’×™ ×”×™×©×•×™×•×ª"
      ],
      "metadata": {
        "id": "mIq0IvWOpUwO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text with spaCy\n",
        "doc = nlp(longest_review_text)\n",
        "\n",
        "# Extract and display named entities and their labels\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: '{ent.text}', Type: {ent.label_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOnDL3lXpVc5",
        "outputId": "a4b509a7-aaf9-4589-a4de-401ca5b0f61c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entity: '11.99', Type: MONEY\n",
            "Entity: 'Subway', Type: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#×—×œ×§ ×©× ×™ : ×™×™×¦×•×’ ×•× ×™×ª×•×— ×‘×™×§×•×¨×•×ª ×¢×œ ×‘×¡×™×¡ ×™×©×•×™×•×ª ××–×•×”×•×ª (NER)"
      ],
      "metadata": {
        "id": "Cb-Nt8ZoqFuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×: ×–×™×”×•×™ ×™×©×•×™×•×ª ×‘×›×œ ×‘×™×§×•×¨×ª"
      ],
      "metadata": {
        "id": "S_Tvi4kKqSYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load your reviews file (adjust path if needed)\n",
        "df = pd.read_csv('/content/drive/MyDrive/Restaurant_Reviews.csv')\n",
        "\n",
        "# Function to extract entities from a single review\n",
        "def extract_entities(text):\n",
        "    doc = nlp(text)\n",
        "    # Return list of (entity text, entity label)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]\n",
        "\n",
        "# Apply to each review\n",
        "df['Entities'] = df['Review'].apply(extract_entities)\n",
        "\n",
        "# Optionally save to CSV for later analysis\n",
        "df.to_csv(\"Reviews_with_Entities.csv\", index=False)\n",
        "\n",
        "# Display a sample\n",
        "print(df[['Review', 'Entities']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-LFHgqoqM_h",
        "outputId": "40f400e8-ed8b-445e-a3f3-2c8522fb3380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  \\\n",
            "0                           Wow... Loved this place.   \n",
            "1                                 Crust is not good.   \n",
            "2          Not tasty and the texture was just nasty.   \n",
            "3  Stopped by during the late May bank holiday of...   \n",
            "4  The selection on the menu was great and so wer...   \n",
            "\n",
            "                                            Entities  \n",
            "0                                                 []  \n",
            "1                                     [(Crust, ORG)]  \n",
            "2                                                 []  \n",
            "3  [(the late May bank, DATE), (Rick Steve, PERSON)]  \n",
            "4                                                 []  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×‘: ×™×¦×™×¨×ª ×™×™×¦×•×’ ×”××‘×•×¡×¡ ×¢×œ ×™×©×•×™×•×ª"
      ],
      "metadata": {
        "id": "Epawb-rosV-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the reviews data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Restaurant_Reviews.csv\")\n",
        "\n",
        "# Progress bar for entity extraction\n",
        "tqdm.pandas()\n",
        "\n",
        "# Extract entities as text\n",
        "def extract_entity_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join(ent.text for ent in doc.ents)\n",
        "\n",
        "df['Entity_Text'] = df['Review'].progress_apply(extract_entity_text)\n",
        "\n",
        "# Build TF-IDF on entity representation\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_entity = vectorizer.fit_transform(df['Entity_Text'])\n",
        "y = df['Liked']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0U5h4eUsmsX",
        "outputId": "24e79578-4b47-419b-e170-7981394726aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:09<00:00, 101.96it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×’: ××™××•×Ÿ ××•×“×œ×™× ×¢×œ ×‘×¡×™×¡ ×™×™×¦×•×’ ×”×™×©×•×™×•×ª"
      ],
      "metadata": {
        "id": "5vd79T6OscsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Split into Train/Test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_entity, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Prepare models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Train each model\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "YD_7TXUjvHeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×“: ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™×"
      ],
      "metadata": {
        "id": "KrRaEEuEse8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "\n",
        "# Evaluate each model\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall   : {rec:.4f}\")\n",
        "    print(f\"F1 Score : {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(pd.DataFrame(cm,\n",
        "                       index=['Actual Negative','Actual Positive'],\n",
        "                       columns=['Predicted Negative','Predicted Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_ucq5XDvtB2",
        "outputId": "074e9731-a936-4299-e351-d8dbe83de093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Logistic Regression ====\n",
            "Accuracy : 0.4900\n",
            "Precision: 0.5625\n",
            "Recall   : 0.0865\n",
            "F1 Score : 0.1500\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  89                   7\n",
            "Actual Positive                  95                   9\n",
            "\n",
            "==== Random Forest ====\n",
            "Accuracy : 0.4950\n",
            "Precision: 0.6000\n",
            "Recall   : 0.0865\n",
            "F1 Score : 0.1513\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  90                   6\n",
            "Actual Positive                  95                   9\n",
            "\n",
            "==== SVM ====\n",
            "Accuracy : 0.4950\n",
            "Precision: 0.6364\n",
            "Recall   : 0.0673\n",
            "F1 Score : 0.1217\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  92                   4\n",
            "Actual Positive                  97                   7\n",
            "\n",
            "==== Multinomial NB ====\n",
            "Accuracy : 0.4900\n",
            "Precision: 0.5625\n",
            "Recall   : 0.0865\n",
            "F1 Score : 0.1500\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  89                   7\n",
            "Actual Positive                  95                   9\n",
            "\n",
            "==== KNN ====\n",
            "Accuracy : 0.5000\n",
            "Precision: 0.6429\n",
            "Recall   : 0.0865\n",
            "F1 Score : 0.1525\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  91                   5\n",
            "Actual Positive                  95                   9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×”: × ×™×ª×•×— ×¨××©×•× ×™"
      ],
      "metadata": {
        "id": "9p0IMlMJskFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All your models ended up with:\n",
        "\n",
        "~49-50% accuracy, barely above random guessing (which for a balanced dataset would be ~50%),\n",
        "\n",
        "extremely low recall (~7-8%) and low F1 (~0.12-0.15).\n",
        "\n",
        "This means:\n",
        "\n",
        "The models almost always predict \"Negative\" (the dominant class in the confusion matrix rows), missing most positives.\n",
        "\n",
        "Very few positive reviews are correctly identified.\n",
        "\n",
        "ğŸ‘‰ This tells us that relying only on named entities (extracted by NER) misses most of the semantic signal needed to classify reviews correctly.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Most of the sentiment in restaurant reviews is expressed through these opinion words, not the entities.\n",
        "So a model only trained on entities loses almost all sentiment-bearing features.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Even though the performance is low, some types of entities could hint at positive experiences:\n",
        "\n",
        "Locations: A mention of popular or upscale areas might be weakly associated with positive experiences.\n",
        "\n",
        "E.g., â€œdinner at Times Squareâ€ might correlate with celebration or special occasions.\n",
        "\n",
        "Organization / Restaurant Names: Known high-end places.\n",
        "\n",
        "MONEY amounts: In some contexts, a review mentioning a higher spend might correlate with a special occasion (though not always positive).\n",
        "\n",
        "EVENTS: Like â€œanniversaryâ€, â€œbirthdayâ€.\n"
      ],
      "metadata": {
        "id": "K4hdTtBTw_VW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#×—×œ×§ ×©×œ×™×©×™ : ×‘× ×™×™×ª ××•×“×œ×™× ×¢×œ ×‘×¡×™×¡ Bag of Words"
      ],
      "metadata": {
        "id": "7bv_KymxzcAm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×: ×”×¡×¨×ª ×¡×™×× ×™ ×¤×™×¡×•×§"
      ],
      "metadata": {
        "id": "Ij-_uIhPzfvM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "# Load the reviews data\n",
        "df = pd.read_csv('/content/drive/MyDrive/Restaurant_Reviews.csv')\n",
        "\n",
        "# Function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Apply to each review\n",
        "df['Clean_Review'] = df['Review'].apply(remove_punctuation)\n",
        "\n",
        "# Show before & after\n",
        "print(df[['Review', 'Clean_Review']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Fv_MTW-0NTY",
        "outputId": "40775af9-2842-4ec4-d7bb-278e5acb4620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  \\\n",
            "0                           Wow... Loved this place.   \n",
            "1                                 Crust is not good.   \n",
            "2          Not tasty and the texture was just nasty.   \n",
            "3  Stopped by during the late May bank holiday of...   \n",
            "4  The selection on the menu was great and so wer...   \n",
            "\n",
            "                                        Clean_Review  \n",
            "0                               Wow Loved this place  \n",
            "1                                  Crust is not good  \n",
            "2           Not tasty and the texture was just nasty  \n",
            "3  Stopped by during the late May bank holiday of...  \n",
            "4  The selection on the menu was great and so wer...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Why remove punctuation?\n",
        "When building Bag of Words (or TF-IDF) models, we tokenize text into words.\n",
        "Punctuation is usually noise for this because:\n",
        "\n",
        "It doesnâ€™t carry meaning for classification tasks like sentiment (punctuation isnâ€™t usually tied to positive/negative opinions).\n",
        "\n",
        "â€œgood,â€ and â€œgoodâ€ would be treated as different tokens if we keep the comma. This unnecessarily increases the feature space and splits frequency counts.\n",
        "\n",
        "By removing punctuation, we standardize the tokens, reducing sparsity and improving the modelâ€™s ability to recognize repeated words."
      ],
      "metadata": {
        "id": "xzs22cKw0piw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×‘: ×”×—×–×¨×ª ××™×œ×™× ×œ×¦×•×¨×ª×Ÿ ×”×‘×¡×™×¡×™×ª"
      ],
      "metadata": {
        "id": "aLZ3xUKjzlC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Lemmatize\n",
        "def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    return \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "df['Lemmatized_Review'] = df['Clean_Review'].progress_apply(lemmatize_text)\n",
        "\n",
        "# Save to new CSV\n",
        "df.to_csv(\"Restaurant_Reviews_Lemmatized.csv\", index=False)\n",
        "\n",
        "# Preview\n",
        "print(df[['Clean_Review', 'Lemmatized_Review']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lkLyF_91khD",
        "outputId": "bd0023f4-f203-4516-8a23-99ee43c909c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:09<00:00, 106.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        Clean_Review  \\\n",
            "0                               Wow Loved this place   \n",
            "1                                  Crust is not good   \n",
            "2           Not tasty and the texture was just nasty   \n",
            "3  Stopped by during the late May bank holiday of...   \n",
            "4  The selection on the menu was great and so wer...   \n",
            "\n",
            "                                   Lemmatized_Review  \n",
            "0                                wow love this place  \n",
            "1                                  Crust be not good  \n",
            "2            not tasty and the texture be just nasty  \n",
            "3  stop by during the late May bank holiday off R...  \n",
            "4  the selection on the menu be great and so be t...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What does â€œreturn to basic formâ€ mean?\n",
        "When building a text model, we want:\n",
        "\n",
        "â€œrunsâ€, â€œrunningâ€, â€œranâ€\n",
        "to all reduce to â€œrunâ€.\n",
        "\n",
        "This helps group related words under a single feature, so the model learns that they refer to the same concept."
      ],
      "metadata": {
        "id": "_h_sarZg2Q3C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###What did we choose and why?\n",
        "Weâ€™ll use: lemmatization\n",
        "Because it ensures valid dictionary words and handles context (verbs vs. nouns) better.\n",
        "\n",
        "Itâ€™s slower, but for understanding restaurant reviews (where opinion words matter), accuracy is worth it."
      ],
      "metadata": {
        "id": "8RWhKSn02ZJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×’: ×™×¦×™×¨×ª ×™×™×¦×•×’ ×‘×©×™×˜×ª Bag of Words"
      ],
      "metadata": {
        "id": "TGsZu0Mfzn-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load the lemmatized data\n",
        "df = pd.read_csv(\"Restaurant_Reviews_Lemmatized.csv\")\n",
        "\n",
        "# Build the Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(df['Lemmatized_Review'])\n",
        "\n",
        "# Convert to DataFrame\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Save to CSV\n",
        "bow_df.to_csv(\"Bag_of_Words_Representation.csv\", index=False)\n",
        "\n",
        "# Show shape and first few rows\n",
        "print(\"Shape of BoW matrix:\", X_bow.shape)\n",
        "print(bow_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fauEv3a9TFR",
        "outputId": "5a4d72e0-6772-41d1-e542-a45ea51b1597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of BoW matrix: (1000, 1755)\n",
            "   10  100  1199  12  15  17  1979  20  2007  23  ...  yelpers  yet  you  \\\n",
            "0   0    0     0   0   0   0     0   0     0   0  ...        0    0    0   \n",
            "1   0    0     0   0   0   0     0   0     0   0  ...        0    0    0   \n",
            "2   0    0     0   0   0   0     0   0     0   0  ...        0    0    0   \n",
            "3   0    0     0   0   0   0     0   0     0   0  ...        0    0    0   \n",
            "4   0    0     0   0   0   0     0   0     0   0  ...        0    0    0   \n",
            "\n",
            "   your  yourself  yucky  yukon  yum  yummy  zero  \n",
            "0     0         0      0      0    0      0     0  \n",
            "1     0         0      0      0    0      0     0  \n",
            "2     0         0      0      0    0      0     0  \n",
            "3     0         0      0      0    0      0     0  \n",
            "4     0         0      0      0    0      0     0  \n",
            "\n",
            "[5 rows x 1755 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the Bag of Words method?\n",
        "Bag of Words is a technique to represent text data numerically.\n",
        "\n",
        "It converts a document (sentence, review, etc.) into a vector that counts how many times each word appears.\n",
        "\n",
        "It ignores grammar, word order, punctuation, and focuses only on the frequency of words."
      ],
      "metadata": {
        "id": "y04t7Q5G9ofI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Why is Bag of Words important?\n",
        "It translates text into numbers so that machine learning models can use it.\n",
        "\n",
        "Helps capture which words are most frequently used in reviews.\n",
        "\n",
        "Often works surprisingly well for sentiment tasks, since words like \"bad\", \"great\", \"dirty\", \"amazing\" directly correlate with labels."
      ],
      "metadata": {
        "id": "3pw--E2I9xpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×“: ××™××•×Ÿ ××•×“×œ×™×"
      ],
      "metadata": {
        "id": "IWKWR5iuzpSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the lemmatized data\n",
        "df = pd.read_csv(\"Restaurant_Reviews_Lemmatized.csv\")\n",
        "\n",
        "# Build Bag of Words\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(df['Lemmatized_Review'])\n",
        "y = df['Liked']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_bow, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"SVM\": SVC(),\n",
        "    \"Multinomial NB\": MultinomialNB(),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Train\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "alm4QZl5-MeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×”: ×”×¢×¨×›×ª ×‘×™×¦×•×¢×™ ×”××•×“×œ×™×"
      ],
      "metadata": {
        "id": "fJQ9b9yTztGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predict and evaluate\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n==== {name} ====\")\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"Accuracy : {acc:.4f}\")\n",
        "    print(f\"Precision: {prec:.4f}\")\n",
        "    print(f\"Recall   : {rec:.4f}\")\n",
        "    print(f\"F1 Score : {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(pd.DataFrame(cm,\n",
        "                       index=['Actual Negative', 'Actual Positive'],\n",
        "                       columns=['Predicted Negative', 'Predicted Positive']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-j5GWgj-k__",
        "outputId": "b0ba20f0-ec75-4313-9fcb-a625f1765fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Logistic Regression ====\n",
            "Accuracy : 0.7600\n",
            "Precision: 0.8111\n",
            "Recall   : 0.7019\n",
            "F1 Score : 0.7526\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  79                  17\n",
            "Actual Positive                  31                  73\n",
            "\n",
            "==== Random Forest ====\n",
            "Accuracy : 0.7750\n",
            "Precision: 0.8041\n",
            "Recall   : 0.7500\n",
            "F1 Score : 0.7761\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  77                  19\n",
            "Actual Positive                  26                  78\n",
            "\n",
            "==== SVM ====\n",
            "Accuracy : 0.7750\n",
            "Precision: 0.8105\n",
            "Recall   : 0.7404\n",
            "F1 Score : 0.7739\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  78                  18\n",
            "Actual Positive                  27                  77\n",
            "\n",
            "==== Multinomial NB ====\n",
            "Accuracy : 0.7650\n",
            "Precision: 0.8132\n",
            "Recall   : 0.7115\n",
            "F1 Score : 0.7590\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  79                  17\n",
            "Actual Positive                  30                  74\n",
            "\n",
            "==== KNN ====\n",
            "Accuracy : 0.6600\n",
            "Precision: 0.6552\n",
            "Recall   : 0.7308\n",
            "F1 Score : 0.6909\n",
            "\n",
            "Confusion Matrix:\n",
            "                 Predicted Negative  Predicted Positive\n",
            "Actual Negative                  56                  40\n",
            "Actual Positive                  28                  76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##×©×œ×‘ ×•: ×‘×—×™×¨×ª ××•×“×œ ×•×”×¡×§×ª ××¡×§× ×•×ª"
      ],
      "metadata": {
        "id": "DKb4UHdhzwvZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis of model performance\n",
        "####Overall\n",
        "Random Forest and SVM show the best balanced performance:\n",
        "\n",
        "*   Both have highest accuracy (0.775) and similar F1 scores (around 0.77).\n",
        "\n",
        "Random Forest slightly edges out SVM on F1, which combines precision + recall.\n",
        "\n",
        "Logistic Regression and Naive Bayes also performed well, but with slightly lower F1.\n",
        "\n",
        "KNN lags clearly behind in accuracy (0.66) and precision (0.65).\n",
        "\n",
        "####Looking deeper at confusion matrices\n",
        "Random Forest and SVM had fewer false positives & false negatives, showing balanced sensitivity (recall) and correctness (precision).\n",
        "\n",
        "KNN had a very high number of misclassifications on negatives (40)."
      ],
      "metadata": {
        "id": "3rw0W_EWKOXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model selection: Random Forest Classifier\n",
        "####Why?\n",
        "It achieved the highest F1 Score (0.7761), meaning it best balances precision & recall â€” which is crucial in sentiment analysis where both false positives and false negatives matter.\n",
        "\n",
        "It also had the highest recall (0.7500) among the top performers, ensuring it catches more positive reviews correctly, while maintaining strong precision.\n",
        "\n",
        "Random Forest is also robust to overfitting, handles noisy data well, and is relatively interpretable"
      ],
      "metadata": {
        "id": "sh5jVE4ZLCdU"
      }
    }
  ]
}